---
output: github_document
always_allow_html: true
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

On rare occasions, the wrong template has been used on a [YSI EXO2](https://www.ysi.com/EXO2) sonde and parameters get loaded into the wrong time series in AQUARIUS Time-Series.  When this happens, we would rather overwrite the data from the wrong parameter with the data from the correct parameter.  The overwriteappend operation in the AQUARIUS Time-Series Acquisition API allows you to completely overwrite data within a time series.  We use the Swagger UI for this process, and the input is a JSON object that holds the unique ID for the time series, the data points, and the time period.  This project prepares a JSON object to be input into the Swagger UI for the AQUARIUS Time-Series Acquisition API.  

The script (available on GitHub [here](OverwriteAppend_PrepDat.R)) requires a telemetry file (.dat), the time series Unique ID, and the start and end time of the period you would like to overwrite with new data.

Note that the script in the GitHub repo is designed to allow users to easily jump to the lines of code that need to be changed for each use by making use of the [Code Sections](https://support.rstudio.com/hc/en-us/articles/200484568-Code-Folding-and-Sections) functionality in RStudio (at least four # in a row at the end of line creates a new Section).

```{r, include = F}
library(readr)         # for read_csv()
library(jsonlite)      # for toJSON()
library(dplyr)
library(knitr)
library(pander)
```

## Input data

The script is designed to accomodate .dat telemetry files downloaded from EXO2 YSI deployments.  The program we use results in the following format:

```{r, message = F}
dat <- read_csv("./dat_files/IRLML02_WQ_Hourly.dat", skip = 1)
```

```{r, results = 'asis', echo = F}
pander(head(dat), style = "rmarkdown", split.tables = Inf)
```

## Clean the data

The structure of the telemetry file requires some very basic data cleaning.  Here we remove the first two rows, which do not contain value data, and convert data types from character to POSIXct and numeric.

```{r}
dat <- dat %>% 
  slice(3:n()) %>% 
  mutate(TIMESTAMP = as.POSIXct(TIMESTAMP, tz = "EST")) %>% 
  mutate_at(vars(RECORD:SUNA_SupVolt), as.numeric)
```


## Subset the data

After identifying the AQUARIUS parameter name and label used for the time series in question, we use a [crosswalk table](TelemetryParameters.csv) of AQUARIUS parameter names and labels with telemetry column names to select the appropriate column from the data frame and rename the columns.  The new column names are required to ensure that the Points section of the JSON object has the correct labels.  Note that you would need to customize the crosswalk table in order to use this in your own AQUARIUS implementation.

```{r}
# Use the Parameter and Label as shown in Aquarius
parameter <- "Sp Cond"
label <- "YSI"

# Load crosswalk table of telemetry file columns and parameter/label combinations
xwalk <- read.csv("TelemetryParameters.csv")

# Get column name
column <- as.character(xwalk$Telemetry_Column[xwalk$Parameter == parameter & 
                                              xwalk$Label == label])

# Subset the dat file to include only the datetime and the parameter of interest
# Rename the columns
dat <- dat %>% 
  select(Time = TIMESTAMP, Value = column)
```

```{r, results = 'asis', echo = F}
pander(head(dat), style = "rmarkdown", split.tables = Inf)
```

After identifying the start and end time for the period requiring overwriting, we subset the data frame further.

```{r}
# Start datetime of period requiring overwrite, inclusive
# Use format shown in the second argument
starttime <- as.POSIXct("2017-08-09 14:00:00", "%Y-%m-%d %H:%M:%S", tz = "EST")

# End datetime of period requiring overwrite, inclusive
# Use format shown in the second argument
endtime <- as.POSIXct("2017-08-10 07:00:00", "%Y-%m-%d %H:%M:%S", tz = "EST")

# Subset the data set based on the these time contraints
cut <- dat %>% 
  filter(Time >= starttime, Time <= endtime)
```

```{r, results = 'asis', echo = F}
pander(head(cut), style = "rmarkdown", split.tables = Inf)
```

## Store timestamps as ISO 8601 in GMT

Timestamps must be stored in ISO 8601 format in GMT in order to be uploaded to the API.

```{r}
cut$Time <- format(cut$Time,"%Y-%m-%dT%H:%M:%SZ", tz = "GMT")
starttime_iso <- format(starttime,"%Y-%m-%dT%H:%M:%SZ", tz = "GMT")
endtime_iso <- format(endtime + 60,"%Y-%m-%dT%H:%M:%SZ", tz = "GMT")
```

The starttime was originally `r starttime`, and after conversion it is `r starttime_iso`.  The endtime follows the same pattern, and the Date column in the dataframe is now formatted correctly for the API.

```{r, results = 'asis', echo = F}
pander(head(cut), style = "rmarkdown", split.tables = Inf)
```

## Get the time series Unique ID

The API makes use of the 32-character time series Unique ID, found by selecting View/Edit Details under the hamburger button for the time series in question and navigating to the bottom of the Time Series Attributes section.

```{r}
uid <- "9259636e1fb9425f9934b355a785d7e4"
```

## Create a data frame that is built around the API's JSON structure requirements

The JSON object that can be used in the API has three distinct parts: a Unique ID, the data Points, and the Time Range.  We first create a nested data frame with the JSON structure.

```{r}
## Part 1: Unique ID
df_json <- data.frame(UniqueID = uid)

## Part 2: Points
df_json$Points <- list(cut)

## Part 3: Time Range
df_json$TimeRange <- data.frame(Start = starttime_iso,
                              End = endtime_iso)

str(df_json)
```

## Convert to a JSON object

Finally, we convert the nested data frame into a JSON object.

```{r}
json_for_export <- jsonlite::toJSON(df_json, pretty = T)
json_for_export
```

## Use the JSON object in the AQUARIUS Time-Series Acquisition API

To execute the overwriteappend operation in the API using Swagger UI, you simply paste the JSON object into the body of the Parameters section, paste the Unique ID and Time Period from the JSON object into their respective boxes, and run it.
