---
output: github_document
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


This project prepares a json object to be input into the Swagger UI for the AQUARIUS Acquisition API.  This script requires a telemetry file (.dat), the time series Unique ID, and the start and end time of the period you would like to overwrite with new data.

Note that the [script] in the GitHub repo is designed to allow users to easily jump to the lines of code that need to be changed for each use by making use of the [Code Sections](https://support.rstudio.com/hc/en-us/articles/200484568-Code-Folding-and-Sections) functionality in RStudio (at least four # in a row at the end of line creates a new Section).

```{r, include = F}
library(readr)         # for read_csv()
library(jsonlite)      # for toJSON()
library(dplyr)
library(knitr)
library(pander)
```

The script is designed to accomodate .dat telemetry files downloaded from EXO2 YSI deployments.  The format of these files is like this:

```{r, message = F}
dat <- read_csv("./dat_files/IRLML02_WQ_Hourly.dat", skip = 1)
```

```{r, results = 'asis', echo = F}
pander(head(dat), style = "rmarkdown", split.tables = Inf)
```

## Data Cleaning

The structure of the telemetry file requires some very basic data cleaning.  Here we remove the first two rows, which do not contain value data, and convert data types from character to POSIXct and numeric.

```{r}
dat <- dat[-c(1:2),]
dat$TIMESTAMP <- as.POSIXct(dat$TIMESTAMP,tz = "EST")
dat[,2:length(dat)] <- sapply(dat[,2:length(dat)],
                              as.numeric)
```


## Subsetting the data

After identifying the AQUARIUS parameter name and label used for the time series in question, we use a crosswalk table of AQUARIUS parameter names and labels with telemetry column names to select the appropriate column from the data frame.

```{r}
# Use the Parameter and Label as shown in Aquarius
parameter <- "Sp Cond"
label <- "YSI"

# Load crosswalk table of telemetry file columns and parameter/label combinations
xwalk <- read.csv("TelemetryParameters.csv")

# Get column name
column <- as.character(xwalk$Telemetry_Column[xwalk$Parameter == parameter & 
                                              xwalk$Label == label])

# Subset the dat file to include only the datetime and the parameter of interest
dat <- dat[,c("TIMESTAMP",column)]
```

```{r, results = 'asis', echo = F}
pander(head(dat), style = "rmarkdown", split.tables = Inf)
```

After identifying the start and end time for the period requiring overwriting, we subset the data frame further, and rename the columns.

```{r}
# Start datetime of period requiring overwrite, inclusive
# Use format shown in the second argument
starttime <- as.POSIXct("2017-08-09 14:00:00", "%Y-%m-%d %H:%M:%S", tz = "EST")

# End datetime of period requiring overwrite, inclusive
# Use format shown in the second argument
endtime <- as.POSIXct("2017-08-10 07:00:00", "%Y-%m-%d %H:%M:%S", tz = "EST")

# Subset the data set based on the these time contraints
cut <- dat[dat$TIMESTAMP >= starttime & dat$TIMESTAMP <= endtime, ]

# Change column names
names(cut) <- c("Time", "Value")
```

```{r, results = 'asis', echo = F}
pander(head(cut), style = "rmarkdown", split.tables = Inf)
```

## Store timestamps as ISO 8601 in GMT

Timestamps must be stored in ISO 8601 format in GMT in order to be uploaded to the API.

```{r}
cut$Time <- format(cut$Time,"%Y-%m-%dT%H:%M:%SZ", tz = "GMT")
starttime_iso <- format(starttime,"%Y-%m-%dT%H:%M:%SZ", tz = "GMT")
endtime_iso <- format(endtime + 60,"%Y-%m-%dT%H:%M:%SZ", tz = "GMT")
```

The starttime was originally `r starttime`, and after conversion it is `r starttime_iso`.  The endtime follows the same pattern, and the Date column in the dataframe is now formatted correctly for the API.

```{r, results = 'asis', echo = F}
pander(head(cut), style = "rmarkdown", split.tables = Inf)
```

## Get the time series Unique ID

The API makes use of the 32-character time series Unique ID, found by selecting View/Edit Details under the hamburger buggon for the time series in question and navigating to the bottom of the Time Series Attributes section.

```{r}
uid <- "9259636e1fb9425f9934b355a785d7e4"
```

## Create a data frame that is built around the json structure requirements as defined by Aquatic Informatics

The JSON object that can be used in the API has three distinct parts: a Unique ID, the data Points, and the Time Range.  We first create a nested data frame with the JSON structure.

```{r}
## Part 1: Unique ID
df_json <- data.frame(UniqueID = uid)

## Part 2: Points
df_json$Points <- list(cut)

## Part 3: Time Range
df_json$TimeRange <- data.frame(Start = starttime_iso,
                              End = endtime_iso)

str(df_json)
```

## Convert to a JSON object

```{r}
json_for_export <- jsonlite::toJSON(df_json, pretty = T)
json_for_export
```



##==> CHANGE JSON FILE LOCATION AND FILE NAME HERE ####
## You MUST include .json at the end of the file name
## Save file to disk
write(json_for_export, file = "./JSON_files/HBI_SpCond.json")

